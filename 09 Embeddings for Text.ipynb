{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
    "\n",
    "Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
    "\n",
    "Additional points may be deducted if these requirements are not met:\n",
    "\n",
    "    \n",
    "* Comment your code;\n",
    "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
    "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 19:12:35.098324: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-12 19:12:35.161390: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-12 19:12:35.386754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 19:12:36.526194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import plotly.graph_objs as plotly  # for interactive plots\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqppUDpmdptk"
   },
   "source": [
    "In this lab, you'll train a <span style=\"color:chocolate\">sentiment</span> classifier for movie reviews. \n",
    "\n",
    "* The input is the text of a movie review;\n",
    "* The output is the probability the input is a positive review.\n",
    "* The target labels are binary, 0 for negative and 1 for positive.\n",
    "\n",
    "The data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words), and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5870,
     "status": "ok",
     "timestamp": 1646684495083,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "s6M-asvhQWV_",
    "outputId": "1aca520b-e1b6-4006-ac44-78ff33ac6da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (25000,)\n",
      "Y_train.shape: (25000,)\n",
      "X_test.shape: (25000,)\n",
      "Y_test.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"Y_train.shape:\", Y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"Y_test.shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example data: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First training example label: 1\n"
     ]
    }
   ],
   "source": [
    "print('First training example data:', X_train[0])\n",
    "print('First training example label:', Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyIWiy-4gQK-"
   },
   "source": [
    "As you can see, the first training example is a positive review. However, that sequence of integer IDs is hard to read. \n",
    "\n",
    "The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1646684508506,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "HQ-qATkhUj7c",
    "outputId": "eea86a69-fe6a-4cdb-ac8c-9307a5118e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest ID: 88587\n"
     ]
    }
   ],
   "source": [
    "# The imdb dataset comes with an index mapping words to integers.\n",
    "# In the index the words are ordered by frequency they occur.\n",
    "index = imdb.get_word_index()\n",
    "\n",
    "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
    "# symbols, we need to add 3 to the index values.\n",
    "index = dict([(key, value+3) for (key, value) in index.items()])\n",
    "\n",
    "# Create a reverse index so we can lookup tokens assigned to each id.\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "reverse_index[1] = '<START>'  # start of input\n",
    "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
    "reverse_index[3] = '<UNUSED>'\n",
    "\n",
    "max_id = max(reverse_index.keys())\n",
    "print('Largest ID:', max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h76-b07ehWNQ"
   },
   "source": [
    "Note that our index (and reverse index) have <span style=\"color:chocolate\">88,587</span> tokens. That's quite <span style=\"color:chocolate\">a large vocabulary</span>! \n",
    "\n",
    "Next, let's write a decoding function for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1646684531998,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "UjobmouHS5Dm",
    "outputId": "29975a48-7fda-4600-bd0c-693c35dd8a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "def decode(token_ids):\n",
    "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
    "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
    "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
    "\n",
    "  # Connect the string tokens with a space.\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# Show the ids corresponding tokens in the first example.\n",
    "print(X_train[0])\n",
    "print(decode(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 1:</span> Text lengths (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g47w5CackGBA"
   },
   "source": [
    "Using the training reviews data, answer the following questions:\n",
    "\n",
    "1. What are the minimum, maximum, and mean lengths of positive and negative reviews?\n",
    "2. Create a histogram to visualize the distribution of positive and negative review lengths. Make sure to provide a descriptive title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive review\n",
      "\tminimum lengths:13\n",
      "\tmaximum lengths:2494\n",
      "\tmean lengths:241.56296\n",
      "negative review\n",
      "\tminimum lengths:11\n",
      "\tmaximum lengths:1571\n",
      "\tmean lengths:235.86432\n"
     ]
    }
   ],
   "source": [
    "positive_reviews = [X_train[i] for i, token_id in enumerate(X_train) if Y_train[i]==1]\n",
    "negative_reviews = [X_train[i] for i, token_id in enumerate(X_train) if Y_train[i]==0]\n",
    "\n",
    "positive_reviews_len = [len(review) for review in positive_reviews]\n",
    "negative_reviews_len = [len(review) for review in negative_reviews]\n",
    "\n",
    "\n",
    "print(f\"positive review\")\n",
    "print(f\"\\tminimum lengths:{np.min(positive_reviews_len)}\")\n",
    "print(f\"\\tmaximum lengths:{np.max(positive_reviews_len)}\")\n",
    "print(f\"\\tmean lengths:{np.mean(positive_reviews_len)}\")\n",
    "\n",
    "print(f\"negative review\")\n",
    "print(f\"\\tminimum lengths:{np.min(negative_reviews_len)}\")\n",
    "print(f\"\\tmaximum lengths:{np.max(negative_reviews_len)}\")\n",
    "print(f\"\\tmean lengths:{np.mean(negative_reviews_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axes[0].hist(positive_reviews_len)\n",
    "axes[0].set_title(\"Positive Reviews\")\n",
    "axes[0].set_xlabel(\"text length\")\n",
    "axes[0].set_ylabel(\"example count\")\n",
    "axes[0].set_xticks(range(0, 2500, 500))\n",
    "\n",
    "axes[1].set_title(\"Negative Reviews\")\n",
    "axes[1].hist(negative_reviews_len)\n",
    "axes[1].set_xlabel(\"text length\")\n",
    "axes[1].set_ylabel(\"example count\")\n",
    "axes[1].set_xticks(range(0, 2000, 500))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 2:</span> Token counts (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3ZE9gpkml3a"
   },
   "source": [
    "Using the training data, create a table listing the counts of positive and negative examples that contain each token provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "8YOYo6d01aWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         positive  negative\n",
      "good         4767      4849\n",
      "bad          1491      4396\n",
      "amazing       868       240\n",
      "boring        301      1205\n",
      "laugh         525       685\n",
      "cry           231       114\n"
     ]
    }
   ],
   "source": [
    "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
    "\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for token in tokens:\n",
    "    token_id = index[token]\n",
    "\n",
    "    positives.append(np.sum([1 for review in positive_reviews if token_id in review]))\n",
    "    negatives.append(np.sum([1 for review in negative_reviews if token_id in review]))\n",
    "\n",
    "\n",
    "token_df = pd.DataFrame(\n",
    "    {\n",
    "        \"positive\": positives, \"negative\": negatives\n",
    "    }, index=tokens)\n",
    "\n",
    "print(token_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Data preprocessing (cont'd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from the review length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, you will add special padding tokens at the end of each review until they are all the same length. You will also truncate all training inputs to a specified length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 3:</span> Reduced length and padding (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the <span style=\"color:green\">NotImplemented</span> parts of the <span style=\"color:chocolate\">truncate_pad_data()</span> function below by following these instructions:\n",
    "\n",
    "1. Restrict the maximum number of tokens by truncating all reviews to a length of 300;\n",
    "2. Append special padding tokens (value = 0) to the end of each review until all reviews are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train[0]: 218\n",
      "Length of X_train_padded[0]: 300\n",
      "[    1    14    22    16    43   530   973  1622  1385    65   458  4468\n",
      "    66  3941     4   173    36   256     5    25   100    43   838   112\n",
      "    50   670 22665     9    35   480   284     5   150     4   172   112\n",
      "   167 21631   336   385    39     4   172  4536  1111    17   546    38\n",
      "    13   447     4   192    50    16     6   147  2025    19    14    22\n",
      "     4  1920  4613   469     4    22    71    87    12    16    43   530\n",
      "    38    76    15    13  1247     4    22    17   515    17    12    16\n",
      "   626    18 19193     5    62   386    12     8   316     8   106     5\n",
      "     4  2223  5244    16   480    66  3785    33     4   130    12    16\n",
      "    38   619     5    25   124    51    36   135    48    25  1415    33\n",
      "     6    22    12   215    28    77    52     5    14   407    16    82\n",
      " 10311     8     4   107   117  5952    15   256     4 31050     7  3766\n",
      "     5   723    36    71    43   530   476    26   400   317    46     7\n",
      "     4 12118  1029    13   104    88     4   381    15   297    98    32\n",
      "  2071    56    26   141     6   194  7486    18     4   226    22    21\n",
      "   134   476    26   480     5   144    30  5535    18    51    36    28\n",
      "   224    92    25   104     4   226    65    16    38  1334    88    12\n",
      "    16   283     5    16  4472   113   103    32    15    16  5345    19\n",
      "   178    32     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "def truncate_pad_data(sequences, max_length):\n",
    "    # Keras has a convenient utility for padding a sequence:\n",
    "    # tf.keras.preprocessing.sequence.pad_sequences()\n",
    "    # Also make sure you get a numpy array rather than an array of lists.\n",
    "    padded_data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\", value=0)\n",
    "    return padded_data\n",
    "\n",
    "# 1+ 2: Truncate and pad the training data\n",
    "X_train_padded = truncate_pad_data(X_train, max_length=300)\n",
    "\n",
    "# Check the padded output.\n",
    "print('Length of X_train[0]:', len(X_train[0]))\n",
    "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you want to be able to limit the vocabulary size. Since the <span style=\"color:chocolate\">truncate_pad_data()</span> function produces fixed-length sequences in a numpy matrix, one can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 4:</span> Reduced vocabulary (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the <span style=\"color:green\">NotImplemented</span> parts of the <span style=\"color:chocolate\">limit_vocab()</span> function below by following these instructions:\n",
    "\n",
    "1. Keep just token ids less than 1000, replacing all others with OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173  36 256\n",
      "   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4 172 112\n",
      " 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192  50  16\n",
      "   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16  43 530\n",
      "  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5  62 386\n",
      "  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130  12  16\n",
      "  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215  28  77\n",
      "  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2   7   2\n",
      "   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13 104  88\n",
      "   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226  22  21\n",
      " 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104   4 226\n",
      "  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19\n",
      " 178  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "<START> this film was just brilliant casting # # story direction # really # the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same # # as myself so i loved the fact there was a real # with this film the # # throughout the film were great it was just brilliant so much that i # the film as soon as it was released for # and would recommend it to everyone to watch and the # # was amazing really # at the end it was so sad and you know what they say if you # at a film it must have been good and this definitely was also # to the two little # that played the # of # and paul they were just brilliant children are often left out of the # # i think because the stars that play them all # up are such a big # for the whole film but these children are amazing and should be # for what they have done don't you think the whole story was so # because it was true and was # life after all that was # with us all # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n"
     ]
    }
   ],
   "source": [
    "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
    "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  reduced_sequences = np.where(sequences < max_token_id, sequences, oov_id)\n",
    "  return reduced_sequences\n",
    "\n",
    "# Reduce vocabulary to 1000 tokens.\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "print(X_train_reduced[0])\n",
    "\n",
    "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
    "print(decode(X_train_reduced[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 5:</span> One-hot encoding (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current feature representations are **sparse**. That is, one only keeps track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
    "\n",
    "To avoid any memory limitations, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). \n",
    "\n",
    "The code below clips each review after 20 tokens and keeps only the first 1000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_one_hot shape: (25000, 20, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Keras has a util to create one-hot encodings.\n",
    "X_train_padded = truncate_pad_data(X_train, max_length=20)\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
    "print('X_train_one_hot shape:', X_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape of the one-hot encoded features. For each of the 25000 training examples, you have a 20 x 1000 matrix. That is, for each of 20 token positions, you have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
    "\n",
    "You can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. You'll get to that soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Baseline model:</span> Logistic regression with one-hot encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296Cnt647b5c"
   },
   "source": [
    "You will start with something familiar -- logistic regression. Since your feature representation is in 2 dimensions (20 x 1000), you need to flatten it to pass it to Keras (remember you did this with the pixel data too). Try two strategies for flattening.\n",
    "\n",
    "1. Flatten by *concatenating* (as you did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position;\n",
    "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "6m6eebM-0dUW"
   },
   "outputs": [],
   "source": [
    "def build_onehot_model(average_over_positions=False):\n",
    "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
    "  # Clear session and remove randomness.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "    \n",
    "  if average_over_positions:\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "  else:\n",
    "    # Concatenate.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "      \n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      activation=\"sigmoid\"         # sigmoid activation for binary classification\n",
    "  ))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
    "                optimizer='adam',             # fancy optimizer\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY3W_1-OSZ2X"
   },
   "source": [
    "Now let's try fitting the model to your training data and check performance metrics on the validation data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOVmajSuMjN6"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.ylim(0,1)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 22140,
     "status": "ok",
     "timestamp": 1646684718388,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "MyE4PgX70_op",
    "outputId": "3de05dfa-f372-4b77-ba43-f212c2f47c1e"
   },
   "outputs": [],
   "source": [
    "model1 = build_onehot_model()\n",
    "\n",
    "# Fit the model.\n",
    "history1 = model1.fit(\n",
    "  x = X_train_one_hot,  # one-hot training data\n",
    "  y = Y_train,          # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "model1.summary()\n",
    "# Convert the return value into a DataFrame so we can see the train loss \n",
    "# and binary accuracy after every epoch.\n",
    "history1 = pd.DataFrame(history1.history)\n",
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 6:</span>  Comparing logistic regression models (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuCh9aQPv7F_"
   },
   "source": [
    "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
    "\n",
    "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
    "2. How many parameters are there in each model?\n",
    "3. Would you say that either model is overfitting? Why or why not?\n",
    "4. Briefly describe how LR-C differs from LR-A. How do you explain the relationship between their respective validation accuracy results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAN5BejHc__"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Improvement over baseline:</span> Logistic regression with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJIBRqK7lsjG"
   },
   "source": [
    "Next, you will train a model that replaces one-hot representations of each token with learned embeddings.\n",
    "\n",
    "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho6uOeCaBs2e"
   },
   "outputs": [],
   "source": [
    "def build_embeddings_model(average_over_positions=False,\n",
    "                           vocab_size=1000,\n",
    "                           sequence_length=20,\n",
    "                           embedding_dim=2):\n",
    "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
    "  # Clear session and remove randomness.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Embedding(\n",
    "      input_dim=vocab_size,\n",
    "      output_dim=embedding_dim,\n",
    "      input_length=sequence_length)\n",
    "  )\n",
    "\n",
    "  if average_over_positions:\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "  else:\n",
    "    # Concatenate.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "      \n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      activation='sigmoid'         # apply the sigmoid function!\n",
    "  ))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyhoEjAiFSNB"
   },
   "source": [
    "You will train this model, this time using the averaging strategy instead of the concatenating strategy for handling the token sequence. You'll look up the embedding vectors for each token and average them to produce a single vector. Then, you'll use this averaged vector to train a logistic regression model to predict the binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1646684762935,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "uYUE5UwkxoU8",
    "outputId": "20a4c237-8c49-4b24-9ea1-e0f325cb9908"
   },
   "outputs": [],
   "source": [
    "model2 = build_embeddings_model(average_over_positions=True,\n",
    "                               vocab_size=1000,\n",
    "                               sequence_length=20,\n",
    "                               embedding_dim=2)\n",
    "history2 = model2.fit(\n",
    "  x = X_train_reduced,  # our sparse padded training data\n",
    "  y = Y_train,          # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "history2 = pd.DataFrame(history2.history)\n",
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 7:</span> Embedding size tuning (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3k__61hFnag"
   },
   "source": [
    "Experiment with embedding sizes {2, 4, 8, 16, 32, 64}, while keeping other settings fixed and using the averaging strategy instead of the concatenating strategy. Then:\n",
    "\n",
    "1. Construct a table showing the training and validation accuracies for each model after 5 training epochs;\n",
    "2. Calculate the number of parameters in each model;\n",
    "3. Analyze whether learned embeddings improve performance compared to one-hot encoding, and explain why this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7t46ZdX2ofd"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2dWOuxqKHA6"
   },
   "source": [
    "### Inspecting Learned Embeddings\n",
    "You will now retrieve the learned embedding parameters from the trained model2 and plot the token embeddings.\n",
    "\n",
    "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. You can use the <span style=\"color:chocolate\">get_weights()</span> function to get a numpy array with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1646684774106,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "bfsbGSwkaFjo",
    "outputId": "d9d74723-012f-464f-a9f3-8a41d8178868"
   },
   "outputs": [],
   "source": [
    "# Display the model layers.\n",
    "display(model2.layers)\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings2 = model2.layers[0].get_weights()[0]\n",
    "display(embeddings2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apPWscNwcXTE"
   },
   "source": [
    "Now you'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1646684778338,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "5RZMTrA0KttL",
    "outputId": "f5ec9b65-84bb-4c28-c673-cf86a87112df"
   },
   "outputs": [],
   "source": [
    "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
    "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
    "  x1 = embeddings[id_start:id_start+count, 0]\n",
    "  x2 = embeddings[id_start:id_start+count, 1]\n",
    "  \n",
    "  # Get the corresponding words from the reverse index (for labeling).\n",
    "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
    "\n",
    "  # Plot with the plotly library.\n",
    "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
    "                        mode='markers', textposition='bottom left',\n",
    "                        hoverinfo='text')\n",
    "  fig = plotly.Figure(data=[data],\n",
    "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
    "                                           hovermode='closest'))\n",
    "  fig.show()\n",
    "\n",
    "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
    "# some rarer words.    \n",
    "plot_2d_embeddings(embeddings2, id_start=500, count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 8:</span> Interpreting embeddings (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Mm8MjRcZ20"
   },
   "source": [
    "Notice that the 2-D embeddings fall in a narrow diagonal band. Answer the following questions:\n",
    "\n",
    "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
    "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'? What are two other surprising results?\n",
    "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
    "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_qAAvvo2y3t"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <span style=\"color:chocolate\">Bonus question: </span> More data and bigger models (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXCitmUvxfwb"
   },
   "source": [
    "Remember how you limited your input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well you can do using more data and bigger models (more parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pieces of code from above, set up and train a model to achieve at least 80% accuracy on both training and validation data. Ensure the following elements are included:\n",
    "\n",
    "1. Truncate and pad input to the desired length;\n",
    "2. Limit the vocabulary to the desired size;\n",
    "3. Set up a model using embeddings;\n",
    "4. Use the averaging strategy rather than the concatenating strategy;\n",
    "5. Add additional layer(s) after the GlobalAveragePooling1D layer and before the output layer;\n",
    "6. Evaluate the model's performance on the test data and comment on its generalization performance. Hint: remember to apply the same preprocessing to the test data. Use the <span style=\"color:chocolate\">model.evaluate()</span> function for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekbJ4sIq2hID"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbG/uKJC3itEUb58OjZyV3",
   "name": "09 Embeddings for Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
