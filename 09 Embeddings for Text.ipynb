{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
    "\n",
    "Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
    "\n",
    "Additional points may be deducted if these requirements are not met:\n",
    "\n",
    "    \n",
    "* Comment your code;\n",
    "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
    "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import plotly.graph_objs as plotly  # for interactive plots\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqppUDpmdptk"
   },
   "source": [
    "In this lab, you'll train a <span style=\"color:chocolate\">sentiment</span> classifier for movie reviews. \n",
    "\n",
    "* The input is the text of a movie review;\n",
    "* The output is the probability the input is a positive review.\n",
    "* The target labels are binary, 0 for negative and 1 for positive.\n",
    "\n",
    "The data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words), and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5870,
     "status": "ok",
     "timestamp": 1646684495083,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "s6M-asvhQWV_",
    "outputId": "1aca520b-e1b6-4006-ac44-78ff33ac6da9"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"Y_train.shape:\", Y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"Y_test.shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First training example data:', X_train[0])\n",
    "print('First training example label:', Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyIWiy-4gQK-"
   },
   "source": [
    "As you can see, the first training example is a positive review. However, that sequence of integer IDs is hard to read. \n",
    "\n",
    "The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1646684508506,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "HQ-qATkhUj7c",
    "outputId": "eea86a69-fe6a-4cdb-ac8c-9307a5118e47"
   },
   "outputs": [],
   "source": [
    "# The imdb dataset comes with an index mapping words to integers.\n",
    "# In the index the words are ordered by frequency they occur.\n",
    "index = imdb.get_word_index()\n",
    "\n",
    "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
    "# symbols, we need to add 3 to the index values.\n",
    "index = dict([(key, value+3) for (key, value) in index.items()])\n",
    "\n",
    "# Create a reverse index so we can lookup tokens assigned to each id.\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "reverse_index[1] = '<START>'  # start of input\n",
    "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
    "reverse_index[3] = '<UNUSED>'\n",
    "\n",
    "max_id = max(reverse_index.keys())\n",
    "print('Largest ID:', max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h76-b07ehWNQ"
   },
   "source": [
    "Note that our index (and reverse index) have <span style=\"color:chocolate\">88,587</span> tokens. That's quite <span style=\"color:chocolate\">a large vocabulary</span>! \n",
    "\n",
    "Next, let's write a decoding function for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1646684531998,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "UjobmouHS5Dm",
    "outputId": "29975a48-7fda-4600-bd0c-693c35dd8a54"
   },
   "outputs": [],
   "source": [
    "def decode(token_ids):\n",
    "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
    "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
    "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
    "\n",
    "  # Connect the string tokens with a space.\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# Show the ids corresponding tokens in the first example.\n",
    "print(X_train[0])\n",
    "print(decode(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 1:</span> Text lengths (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g47w5CackGBA"
   },
   "source": [
    "Using the training reviews data, answer the following questions:\n",
    "\n",
    "1. What are the minimum, maximum, and mean lengths of positive and negative reviews?\n",
    "2. Create a histogram to visualize the distribution of positive and negative review lengths. Make sure to provide a descriptive title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 2:</span> Token counts (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3ZE9gpkml3a"
   },
   "source": [
    "Using the training data, create a table listing the counts of positive and negative examples that contain each token provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YOYo6d01aWI"
   },
   "outputs": [],
   "source": [
    "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Data preprocessing (cont'd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from the review length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, you will add special padding tokens at the end of each review until they are all the same length. You will also truncate all training inputs to a specified length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 3:</span> Reduced length and padding (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the <span style=\"color:green\">NotImplemented</span> parts of the <span style=\"color:chocolate\">truncate_pad_data()</span> function below by following these instructions:\n",
    "\n",
    "1. Restrict the maximum number of tokens by truncating all reviews to a length of 300;\n",
    "2. Append special padding tokens (value = 0) to the end of each review until all reviews are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad_data(sequences, max_length):\n",
    "    # Keras has a convenient utility for padding a sequence:\n",
    "    # tf.keras.preprocessing.sequence.pad_sequences()\n",
    "    # Also make sure you get a numpy array rather than an array of lists.\n",
    "    padded_data = NotImplemented\n",
    "    return padded_data\n",
    "\n",
    "# 1+ 2: Truncate and pad the training data\n",
    "X_train_padded = truncate_pad_data(X_train, max_length=300)\n",
    "\n",
    "# Check the padded output.\n",
    "print('Length of X_train[0]:', len(X_train[0]))\n",
    "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you want to be able to limit the vocabulary size. Since the <span style=\"color:chocolate\">truncate_pad_data()</span> function produces fixed-length sequences in a numpy matrix, one can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 4:</span> Reduced vocabulary (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the <span style=\"color:green\">NotImplemented</span> parts of the <span style=\"color:chocolate\">limit_vocab()</span> function below by following these instructions:\n",
    "\n",
    "1. Keep just token ids less than 1000, replacing all others with OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
    "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  reduced_sequences = NotImplemented\n",
    "  return reduced_sequences\n",
    "\n",
    "# Reduce vocabulary to 1000 tokens.\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "print(X_train_reduced[0])\n",
    "\n",
    "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
    "print(decode(X_train_reduced[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 5:</span> One-hot encoding (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current feature representations are **sparse**. That is, one only keeps track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
    "\n",
    "To avoid any memory limitations, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). \n",
    "\n",
    "The code below clips each review after 20 tokens and keeps only the first 1000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has a util to create one-hot encodings.\n",
    "X_train_padded = truncate_pad_data(X_train, max_length=20)\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
    "print('X_train_one_hot shape:', X_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape of the one-hot encoded features. For each of the 25000 training examples, you have a 20 x 1000 matrix. That is, for each of 20 token positions, you have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
    "\n",
    "You can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. You'll get to that soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Baseline model:</span> Logistic regression with one-hot encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296Cnt647b5c"
   },
   "source": [
    "You will start with something familiar -- logistic regression. Since your feature representation is in 2 dimensions (20 x 1000), you need to flatten it to pass it to Keras (remember you did this with the pixel data too). Try two strategies for flattening.\n",
    "\n",
    "1. Flatten by *concatenating* (as you did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position;\n",
    "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m6eebM-0dUW"
   },
   "outputs": [],
   "source": [
    "def build_onehot_model(average_over_positions=False):\n",
    "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
    "  # Clear session and remove randomness.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "    \n",
    "  if average_over_positions:\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "  else:\n",
    "    # Concatenate.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "      \n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      activation=\"sigmoid\"         # sigmoid activation for binary classification\n",
    "  ))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
    "                optimizer='adam',             # fancy optimizer\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY3W_1-OSZ2X"
   },
   "source": [
    "Now let's try fitting the model to your training data and check performance metrics on the validation data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOVmajSuMjN6"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.ylim(0,1)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 22140,
     "status": "ok",
     "timestamp": 1646684718388,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "MyE4PgX70_op",
    "outputId": "3de05dfa-f372-4b77-ba43-f212c2f47c1e"
   },
   "outputs": [],
   "source": [
    "model1 = build_onehot_model()\n",
    "\n",
    "# Fit the model.\n",
    "history1 = model1.fit(\n",
    "  x = X_train_one_hot,  # one-hot training data\n",
    "  y = Y_train,          # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "model1.summary()\n",
    "# Convert the return value into a DataFrame so we can see the train loss \n",
    "# and binary accuracy after every epoch.\n",
    "history1 = pd.DataFrame(history1.history)\n",
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 6:</span>  Comparing logistic regression models (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuCh9aQPv7F_"
   },
   "source": [
    "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
    "\n",
    "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
    "2. How many parameters are there in each model?\n",
    "3. Would you say that either model is overfitting? Why or why not?\n",
    "4. Briefly describe how LR-C differs from LR-A. How do you explain the relationship between their respective validation accuracy results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAN5BejHc__"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Improvement over baseline:</span> Logistic regression with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJIBRqK7lsjG"
   },
   "source": [
    "Next, you will train a model that replaces one-hot representations of each token with learned embeddings.\n",
    "\n",
    "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho6uOeCaBs2e"
   },
   "outputs": [],
   "source": [
    "def build_embeddings_model(average_over_positions=False,\n",
    "                           vocab_size=1000,\n",
    "                           sequence_length=20,\n",
    "                           embedding_dim=2):\n",
    "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
    "  # Clear session and remove randomness.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Embedding(\n",
    "      input_dim=vocab_size,\n",
    "      output_dim=embedding_dim,\n",
    "      input_length=sequence_length)\n",
    "  )\n",
    "\n",
    "  if average_over_positions:\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "  else:\n",
    "    # Concatenate.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "      \n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      activation='sigmoid'         # apply the sigmoid function!\n",
    "  ))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyhoEjAiFSNB"
   },
   "source": [
    "You will train this model, this time using the averaging strategy instead of the concatenating strategy for handling the token sequence. You'll look up the embedding vectors for each token and average them to produce a single vector. Then, you'll use this averaged vector to train a logistic regression model to predict the binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1646684762935,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "uYUE5UwkxoU8",
    "outputId": "20a4c237-8c49-4b24-9ea1-e0f325cb9908"
   },
   "outputs": [],
   "source": [
    "model2 = build_embeddings_model(average_over_positions=True,\n",
    "                               vocab_size=1000,\n",
    "                               sequence_length=20,\n",
    "                               embedding_dim=2)\n",
    "history2 = model2.fit(\n",
    "  x = X_train_reduced,  # our sparse padded training data\n",
    "  y = Y_train,          # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "history2 = pd.DataFrame(history2.history)\n",
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 7:</span> Embedding size tuning (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3k__61hFnag"
   },
   "source": [
    "Experiment with embedding sizes {2, 4, 8, 16, 32, 64}, while keeping other settings fixed and using the averaging strategy instead of the concatenating strategy. Then:\n",
    "\n",
    "1. Construct a table showing the training and validation accuracies for each model after 5 training epochs;\n",
    "2. Calculate the number of parameters in each model;\n",
    "3. Analyze whether learned embeddings improve performance compared to one-hot encoding, and explain why this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7t46ZdX2ofd"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2dWOuxqKHA6"
   },
   "source": [
    "### Inspecting Learned Embeddings\n",
    "You will now retrieve the learned embedding parameters from the trained model2 and plot the token embeddings.\n",
    "\n",
    "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. You can use the <span style=\"color:chocolate\">get_weights()</span> function to get a numpy array with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1646684774106,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "bfsbGSwkaFjo",
    "outputId": "d9d74723-012f-464f-a9f3-8a41d8178868"
   },
   "outputs": [],
   "source": [
    "# Display the model layers.\n",
    "display(model2.layers)\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings2 = model2.layers[0].get_weights()[0]\n",
    "display(embeddings2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apPWscNwcXTE"
   },
   "source": [
    "Now you'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1646684778338,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "5RZMTrA0KttL",
    "outputId": "f5ec9b65-84bb-4c28-c673-cf86a87112df"
   },
   "outputs": [],
   "source": [
    "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
    "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
    "  x1 = embeddings[id_start:id_start+count, 0]\n",
    "  x2 = embeddings[id_start:id_start+count, 1]\n",
    "  \n",
    "  # Get the corresponding words from the reverse index (for labeling).\n",
    "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
    "\n",
    "  # Plot with the plotly library.\n",
    "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
    "                        mode='markers', textposition='bottom left',\n",
    "                        hoverinfo='text')\n",
    "  fig = plotly.Figure(data=[data],\n",
    "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
    "                                           hovermode='closest'))\n",
    "  fig.show()\n",
    "\n",
    "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
    "# some rarer words.    \n",
    "plot_2d_embeddings(embeddings2, id_start=500, count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 8:</span> Interpreting embeddings (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Mm8MjRcZ20"
   },
   "source": [
    "Notice that the 2-D embeddings fall in a narrow diagonal band. Answer the following questions:\n",
    "\n",
    "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
    "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'? What are two other surprising results?\n",
    "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
    "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_qAAvvo2y3t"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <span style=\"color:chocolate\">Bonus question: </span> More data and bigger models (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXCitmUvxfwb"
   },
   "source": [
    "Remember how you limited your input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well you can do using more data and bigger models (more parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pieces of code from above, set up and train a model to achieve at least 80% accuracy on both training and validation data. Ensure the following elements are included:\n",
    "\n",
    "1. Truncate and pad input to the desired length;\n",
    "2. Limit the vocabulary to the desired size;\n",
    "3. Set up a model using embeddings;\n",
    "4. Use the averaging strategy rather than the concatenating strategy;\n",
    "5. Add additional layer(s) after the GlobalAveragePooling1D layer and before the output layer;\n",
    "6. Evaluate the model's performance on the test data and comment on its generalization performance. Hint: remember to apply the same preprocessing to the test data. Use the <span style=\"color:chocolate\">model.evaluate()</span> function for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekbJ4sIq2hID"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbG/uKJC3itEUb58OjZyV3",
   "name": "09 Embeddings for Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
