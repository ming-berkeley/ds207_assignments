{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
    "\n",
    "Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
    "\n",
    "Additional points may be deducted if these requirements are not met:\n",
    "    \n",
    "* Comment your code;\n",
    "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
    "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab continues our study of linear regression. You'll train your first models with Tensorflow, using a real dataset to predict car prices from their features. Note that Tensorflow is a rapidly changing library. This means you'll often see warnings about deprecations. You can ignore the warnings in our labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHLcriKWLRe4"
   },
   "source": [
    "You'll use the [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile)  from 1985 Ward's Automotive Yearbook that is part of the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_auto_data_set_code"
   },
   "outputs": [],
   "source": [
    "# Provide the names for the feature columns since the CSV file with the data \n",
    "# does not have a header row.\n",
    "cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n",
    "        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n",
    "        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n",
    "        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n",
    "        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "\n",
    "# Load the data from a CSV file into a pandas dataframe. Remember that each row\n",
    "# is an example and each column in a feature.\n",
    "car_data_init = pd.read_csv(\n",
    "    'https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n",
    "    sep=',', names=cols, header=None, encoding='latin-1')\n",
    "\n",
    "# Display top five rows\n",
    "print('Shape of data:', car_data_init.shape)\n",
    "car_data_init.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is essential for preparing the data in a format that is suitable for ML algorithms. It helps ensure data quality and improvements in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 1:</span> Column selection (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, you will:\n",
    "\n",
    "1. Retain only the following columns: ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']. Name the new dataframe *car_data*.\n",
    "2. Display the data type of each column;\n",
    "3. Convert the data type of each columns to numeric. Coerce missing values to NaN. Hint: use <span style=\"color:chocolate\">pd.to_numeric()</span> method;\n",
    "4. Display the data type of each column after the transformation performed at point 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 2:</span> Example (row) selection (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple again, you will:\n",
    "\n",
    "1. Print the shape of the car_data;\n",
    "\n",
    "2. Remove examples (rows) that have missing value(s). Note that in doing so, you will overwrite the car_data dataset. You should end up with 197 examples after this cleaning.\n",
    "\n",
    "3. Print the shape of the car_data again.\n",
    "\n",
    "It's important to acknowledge that there are multiple approaches to handling missing features, and simply discarding examples with any missing feature, though straightforward, may not be the most optimal solution. However, for the sake of simplicity, you will implement this strategy in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 3:</span> Data shuffling (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you'll be using Batch Gradient Descent (BGD) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative. Note that the original data (above) appears sorted by *make* in alphabetic order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NumPy and Pandas methods:\n",
    "\n",
    "1. Create a list of indices corresponding to the rows in the car_data dataset. Call this list *indices*. Print this list;\n",
    "\n",
    "2. Shuffle *indices* using the <span style=\"color:chocolate\">np.random.permutation()</span> method. Call the resulting array *shuffled_indices*. Print this array;\n",
    "    \n",
    "3. Use the method <span style=\"color:chocolate\">dataframe.reindex()</span> to change the ordering of the car_data dataset based on the order in the *shuffled_indices* array. Note that in doing so, you will overwrite the original dataset. Print the top 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 4:</span> Define outcome and features (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two dataframes as follows:\n",
    "\n",
    "1. The first dataframe contains our outcome of interest: ['price']. Note, this is what we are aiming to predict. Name this dataframe Y. Print shape of Y.\n",
    "2. The second dataframe contains our features of interest: ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']. Name this dataframe X. Print shape of X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 5:</span> Data splits (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <span style=\"color:chocolate\">train_test_split()</span> method available in scikit-learn:\n",
    "1. Partition the (X, Y) data into training, validation, and test sets using a splitting rule of [60%, 20%, 20%], with a random state set to 1234. Name the resulting dataframes as follows: X_train, X_val, X_test, Y_train, Y_val, Y_test. Hint: To create these three partitions you will utilize the train_test_split() method twice. You should obtain [119, 40, 40] examples for training, validation, and test, respectively.\n",
    "2. Print the shape of each dataframe.\n",
    "\n",
    "Note: The validation set is crucial for evaluating different hyperparameter configurations and selecting those that yield optimal model performance. This approach avoids utilizing the test dataset during model training, as it is assumed to be \"unknown\" at that stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 6:</span> Data standardization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this concept in mind, complete the following tasks:\n",
    "\n",
    "1. Output the quantile values (0.25, 0.5, 0.75, 0.95) for all features in the X_train dataset. Are these values uniformly scaled across features?\n",
    "\n",
    "2. Standardize all features in X_train, X_val, and X_test. Label the resulting dataframes as X_train_std, X_val_std, and X_test_std, respectively. Hint: standardize the validation and test data using the mean and standard deviation computed from the training data. Why?\n",
    "\n",
    "3. Similar to point 2. but now standardize the outcome variable. Label the resulting dataframes as Y_train_std, Y_val_std, and Y_test_std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA plays a very important role in ML. The goal here is to develop a good understanding of our dataset, identify any data quality issues, understand patterns and relationships, which in turn, aids in subsequent modeling and interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 7:</span> Scatterplot matrix (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will use some simple yet useful techniques to visualize the distribution of the data. \n",
    "\n",
    "Let's start with:\n",
    "\n",
    "1. A scatterplot matrix to visualize the pair-wise correlations between different features and outcome in the (X_train_std, Y_train_std) data. You will use the <span style=\"color:chocolate\">sns.pairplot()</span> method from the seaborn library imported at the top of the notebook;\n",
    "2. Is any of the variables in the data normally distributed? Is it necessary for the explanatory or target variable to be normally distributed in order to train a ML model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 8:</span> Correlation matrix (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will:\n",
    "\n",
    "1. Plot a correlation matrix in the form of a heatmap to visualize the linear relationships between different features and outcome in the (X_train_std, Y_train) data. Hint: this example here is very useful: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "    \n",
    "2. Answer the following questions: \n",
    " - Which two features are likely to be most redundant?\n",
    " - Which feature is likely to be least useful for predicting price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 9:</span> Baseline model (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by evaluating a baseline model. Precisely, you'll use the average price of cars in the training set as our baseline model -- that is, the baseline always predicts the average price regardless of the input.\n",
    "\n",
    "1. Implement this baseline using the Y_train_std data and print the average price. Note: You can revert the price variable to the original scale for interpretation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBXeXWygp4T"
   },
   "source": [
    "### <span style=\"color:chocolate\">Exercise 10:</span> Improvement over Baseline with TensorFlow (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDsxLnljlp0C"
   },
   "source": [
    "Let's train a linear regression model much like we did in the previous assignment, but this time using TensorFlow. \n",
    "\n",
    "1. Fill in the <span style=\"color:green\">NotImplemented</span> parts of the build_model() function below by following the instructions provided as comments. Hint: refer to Demo 3 in [bCourses/Modules/Live Session Demos](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) for an example.\n",
    "2. Build and compile a model using the build_model() function and the (X_train_std, Y_train_std) data. Set learning_rate = 0.0001. Call the resulting object *model_tf*.\n",
    "3. Train *model_tf* using the (X_train_std, Y_train_std) data. Set num_epochs = 5. Pass the (X_val_std, Y_val_std) data for validation. Hint: see the documentation behind the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method.\n",
    "3. Generate a plot with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title. Hint: check what the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method returns.\n",
    "\n",
    "More notes on point 1: the idea is to build a *computational graph* for linear regression, and then send data through it. There are many ways to build graphs, but [TenforFlow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras) is recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfdRzjk-RgpG"
   },
   "outputs": [],
   "source": [
    "def build_model(num_features, learning_rate):\n",
    "  \"\"\"Build a TF linear regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    num_features: The number of input features.\n",
    "    learning_rate: The desired learning rate for SGD.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  # This is not strictly necessary, but each time you build a model, TF adds\n",
    "  # new nodes (rather than overwriting), so the colab session can end up\n",
    "  # storing lots of copies of the graph when you only care about the most\n",
    "  # recent. Also, as there is some randomness built into training with SGD,\n",
    "  # setting a random seed ensures that results are the same on each identical\n",
    "  # training run.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  # Build a model using keras.Sequential. While this is intended for neural\n",
    "  # networks (which may have multiple layers), we want just a single layer for\n",
    "  # linear regression.\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=NotImplemented,        # output dim\n",
    "      input_shape=NotImplemented,  # input dim\n",
    "      use_bias=True,               # use a bias (intercept) param\n",
    "      kernel_initializer=NotImplemented,  # initialize params to 1\n",
    "      bias_initializer=NotImplemented,    # initialize bias to 1\n",
    "  ))\n",
    "\n",
    "  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch SGD\n",
    "  optimizer = NotImplemented\n",
    "\n",
    "  # Finally, compile the model. This finalizes the graph for training.\n",
    "  # We specify the loss and the optimizer above\n",
    "  NotImplemented\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# 2. Build and compile model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 3. Fit the model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a crucial step in optimizing ML models. It involves systematically adjusting hyperparameters such as learning rate, number of epochs, and optimizer to find the model configuration that leads to the best generalization performance.\n",
    "\n",
    "This tuning process is typically conducted by monitoring the model's performance on the validation vs. training set. It's important to note that using the test set for hyperparameter tuning can compromise the integrity of the evaluation process by violating the assumption of \"blindness\" of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 11:</span> Hyperparameter tuning (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fine-tune the hyperparameters of *model_tf* to determine the setup that yields the most optimal generalization performance. Feel free to explore various values for the hyperparameters. Hint: ask your instructors and TAs for help if in doubt.\n",
    "\n",
    "After identifying your preferred model configuration, print the following information:\n",
    "\n",
    "2. The learned parameters of the model (this should include the bias term). Hint: use  <span style=\"color:chocolate\">model_tf.layers[0].get_weights()</span>.\n",
    "3. The loss at the final epoch on both the training and validation datasets;\n",
    "4. The percentage difference between the losses observed on the training and validation datasets.\n",
    "\n",
    "\n",
    "Please note that we will consider 'optimal model configuration' any last-epoch loss that is below 0.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Evaluation and Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you've determined the optimal set of hyperparameters, it's time to evaluate your optimized model on the test data to gauge its performance in real-world scenarios, commonly known as inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 12:</span> Computing MSE (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate MSE on both (X_train_std, Y_train_std) and (X_test_std, Y_test_std) datasets. Hint: You can utilize the <span style=\"color:chocolate\">model.evaluate()</span> method provided by tf.keras.\n",
    "\n",
    "2. Does the model demonstrate strong generalization capabilities? Provide an explanation based on your observations.\n",
    "\n",
    "4. Generate a plot to visualize the accuracy of the predictions. Plot the actual (observed) Y_test values on the x-axis and the predicted Y_test values on the y-axis. Additionally, include a 45-degree line in the plot for reference. Ensure that the plot contains appropriate axis labels and a title. Provide commentary on the model's fit based on this visualization. Hint: You can utilize the <span style=\"color:chocolate\">model.predict()</span> method available in tf.keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <span style=\"color:chocolate\">Bonus question</span> (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 12, you reported an aggregated MSE. Let's revisit the exercise by:\n",
    "\n",
    "1. Conducting a subgroup model evaluation. More precisely, compute the test data MSE based on various car subgroups such as make, engine size, fuel type, etc.\n",
    "\n",
    "2. Answering the question: is the model \"fair\" to your chosen car subgroups in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "copyright"
   ],
   "name": "03 Linear Regression with Tensorflow.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
